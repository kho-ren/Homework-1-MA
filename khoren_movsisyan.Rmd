---
title: "Marketing Analytics - Homework 1"
author: "Khoren Movsisyan"
date: "2024-02-24"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


1. Chosen Invention: Robot Coworker - "Sanctuary AI Phoenix"

2. Look-alike innovation from the past: Amazon Echo - "A smart speaker with voice control."

For a product like the Sanctuary AI Phoenix, the market is relatively new and there is not much data that would be enough for this task. The only related data I was able to found were not for a single product or product line, but rather the whole robotics industry. That is why I decided to take a Amazon Echo, for which there is plenty of data, as a look-alike product. 

Justification:
While Robot Coworker and Amazon Echo are not the same product and have a lot of differences in terms of their specific functions, they share some key underlying principles. In a sense, Amazon Echo and similar products have paved the way for innovations like Sanctuary AI Phoenix in the future. Let me elaborate on the shared characteristics.

1) Both Amazon Echo and Sanctuary AI Phoenix have voice interaction as a primary mode of communication with the user.
2) Both products have AI incorporated in their core, and use it to function, to understand and respond to user commands, among other tasks. 
3) The success and the adoption of voice assistant speakers like Amazon Echo demonstrate users' willingness to this kind of technologies. Users might be more open to embracing Robot Coworkers if they offer similar ease of use and clear benefits.

The connection between these two innovations can help us understand the potential impact of Robot Coworkers and how lessons learned from past inventions can influence their development and future success.



3. Data for the look-alike innovation:

The data chosen for the look-alike innovation is the number of units sold worldwide. The data includes sales from 2014 until 2023. Originally the data contained predicted sales for the years 2024 and 2025, however I have removed them from the calculation. There are two columns in the data, the first being the year and the second one being the units sold in millions of units.


```{r, echo=FALSE}
libs<-c('ggplot2','ggpubr','knitr','diffusion','readxl')

load_libraries<-function(libs){
new_libs <- libs[!(libs %in% installed.packages()[,"Package"])]
if(length(new_libs)>0) {install.packages(new_libs)}
lapply(libs, library, character.only = TRUE)
}
load_libraries(libs)
```

```{r}
data <- read_excel("amazon_echo_data.xlsx", sheet = "Data")
data <- head(data, n = -2)

data_with_predictions <- read_excel("amazon_echo_data.xlsx", sheet = "Data") 
#This will be needed to compare the outputs of the 2 methods used

data
```


```{r}
colnames(data) <- c("Year", "Units")

data$Year <- as.numeric(data$Year)

data
```

Here with the bar plot we can visually see the distribution of the sold units by each year. 

```{r}
units_sales <- ggplot(data, aes(Year, Units)) + 
  geom_bar(stat='identity') + 
  ggtitle("Echo Units sold in mln units")
units_sales
```

4. Estimate Bass model parameters for the look-alike innovation.

Here I have used two methods for the model predictions, first by using the nonlinear least squares method, and then using the diffusion library.

Method 1 - NLS:

```{r}
units <- data$Units
t <- 1:length(units)

estimation_by_nls <- nls(units ~ m * (((p+q)^2/p) * exp(-(p+q)*t)) / (1+(q/p) * exp(-(p+q)*t)) ^ 2, 
              start = c(list(m=sum(units), p=0.02, q=0.4)))
summary(estimation_by_nls)
```

Method 2 - 'diffusion' library

```{r}
estimation_by_diff = diffusion(units)

p=round(estimation_by_diff$w,4)[1]
q=round(estimation_by_diff$w,4)[2]
m=round(estimation_by_diff$w,4)[3]
estimation_by_diff
```



5. Parameter estimation and prediction:

```{r}
bass.f <- function(t,p,q){
    ((p+q)^2/p)*exp(-(p+q)*t)/
    (1+(q/p)*exp(-(p+q)*t))^2
}
```

```{r}
bass.F <- function(t,p,q){
    (1-exp(-(p+q)*t))/
    (1+(q/p)*exp(-(p+q)*t))
}
```

Parameters from nls():
```{r}
ft_nls <- ggplot(data.frame(t=c(1:9)), aes(t)) + 
  stat_function(fun = bass.f, args = c(p=0.006967, q=0.3986)) +  
  ggtitle('f(t)')
ggarrange(ft_nls, units_sales )
```

Parameters from the diffusion library Bass model:
```{r}
ft_diff <- ggplot(data.frame(t=c(1:9)), aes(t)) + 
  stat_function(fun = bass.f, args = c(p=0.0083, q=0.4534)) +  
  ggtitle('f(t)')
ggarrange(ft_diff, units_sales)
```
Now going back to the original data, where we had predicted sales for the years 2024 and 2025, let's visualize it.

```{r}
colnames(data_with_predictions) <- c("Year", "Units")

data_with_predictions$Year <- as.numeric(data_with_predictions$Year)

unit_sales_with_prediction <- ggplot(data_with_predictions, aes(Year, Units)) + 
  geom_bar(stat='identity') + 
  ggtitle("Echo Units sold in mln units with 2024 and 2025 predictions")

unit_sales_with_prediction
```
It becomes apparent then that with the parameters derived by the Nonlinear Least Squares method, we are able to better approximate the given data. That said, I will use those parameters, which are p=0.006967, q=0.3986 and M = 935.7 to do the prediction. 

We can also vizualize the cumulative adoptions of the Echo units:

```{r}
cum_adoptions = ggplot(data.frame(t = c(1, 9)), aes(t)) +
stat_function(fun = bass.F, args = c(p=0.006967, q=0.3986)) +
labs(title = 'Cumulative adoptions of Echo units')
cum_adoptions
```

Prediction:

```{r}
data_with_predictions$Pred_sales = bass.f(1:11, p=0.006967, q=0.3986) * 935.7
ggplot(data_with_predictions, aes(Year, Units)) + geom_bar(stat = 'identity') + 
  geom_point(mapping = aes(Year, Pred_sales), color = 'red')
```

Here is my approach to estimate the number of adopters by period for the industrial robots market.

We know that in 2023 there were about 4.5 million robot units sold, and also that from 2016-2023 a total of 28.74 mln industrial and service robots have been sold.

There are about 334 million companies worldwide. We can assume, hypothetically,
that the number of companies that will want to sometime in the future use robot coworkers to be about 20% of the total number. We get 66.8mln companies that could use industrial robots. 
We will also need to assume/predict an Average Adoption Rate per Adopter. Let's assume each potential adopter purchases an average of 10 industrial robots.



